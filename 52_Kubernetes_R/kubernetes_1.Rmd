---
title: "Untitled"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Source: https://medium.com/google-cloud/deploying-docker-images-to-google-cloud-using-kubernetes-engine-637af009e594
[link](www.rstudio.com)
***
> block quote 

# Requirements
This post assumes that you already have a working docker image, that you can build and run on your local machine, and obviously a Google Cloud account with all of the necessary APIs activated, and the Google Cloud SDK installed on your machine. If all of that is in order, you are good to go!

Source: https://www.r-bloggers.com/how-to-deploy-a-predictive-service-to-kubernetes-with-r-and-the-azurecontainers-package/

# Fit the model
We’ll fit a simple model for illustrative purposes, using the Boston housing dataset (which ships with R in the MASS package). To make the deployment process more interesting, the model we fit will be a random forest, using the randomForest package. This is not part of R, so we’ll have to install it from CRAN.
```{r eval=F}
data(Boston, package="MASS")
install.packages("randomForest")
library(randomForest)

# train a model for median house price
bos_rf <- randomForest(medv ~ ., data=Boston, ntree=100)

# save the model
saveRDS(bos_rf, "bos_rf.rds")
```


# Scoring script for plumber
Now that we have the model, we also need a script to obtain predicted values from it given a set of inputs:
```{r eval=F}
# save as bos_rf_score.R

bos_rf <- readRDS("bos_rf.rds")
library(randomForest)

#* @param df data frame of variables
#* @post /score
function(req, df)
{
    df <- as.data.frame(df)
    predict(bos_rf, df)
}
```


This is fairly straightforward, but the comments may require some explanation. They are plumber annotations that tell it to call the function if the server receives a HTTP POST request with the path /score, and query parameter df. The value of the df parameter is then converted to a data frame, and passed to the randomForest predict method. For a fuller description of how Plumber works, see the Plumber website.[https://rplumber.io/]

# Create a Dockerfile
Let’s package up the model and the scoring script into a Docker image. A Dockerfile to do this is shown below. This uses the base image supplied by Plumber (trestletech/plumber), installs randomForest, and then adds the model and the above scoring script. Finally, it runs the code that will start the server and listen on port 8000.

# example Dockerfile to expose a plumber service

FROM trestletech/plumber

# install the randomForest package
RUN R -e 'install.packages(c("randomForest"))'

# copy model and scoring script
RUN mkdir /data
COPY bos_rf.rds /data
COPY bos_rf_score.R /data
WORKDIR /data

# plumb and run server
EXPOSE 8000
ENTRYPOINT ["R", "-e", \
    "pr <- plumber::plumb('/data/bos_rf_score.R'); \
    pr$run(host='0.0.0.0', port=8000)"]
    
```{bash eval=F}
docker build -t myimage .
```

# Test it

#Push it to Google CLoud

Before pushing it to Google Cloud, we must quickly tag our image with the necessary information that Google Cloud needs. Make sure you replace <PROJECT NAME> with the correct project ID in the Google Cloud dashboard. Issue:
```{bash eval=F}
docker tag myimage gcr.io/virtual-machine-196412/myimage:v1
```

Bash in terminal

```
$docker login -u _json_key -p "$(cat OneDrive/7_DataScience/02_CloudComputing/X_Secrets/GCP/auth.json)" https://gcr.io
```
$docker login -u _json_key -p "$(cat OneDrive/7_DataScience/02_CloudComputing/X_Secrets/GCP/auth.json)" https://gcr.io



```{bash eval=F}
docker push gcr.io/virtual-machine-196412/myimage:v1
```


```{bash eval=F}
gcloud docker — push gcr.io/virtual-machine-196412/myimage:v1
```

Now if you head to your Google Cloud Console, and over to the Container Registry, you should see your new Docker image there. Good work!

We now need to get our image served in Kubernetes Engine. Kubernetes Engine is Google’s hosted version of Kubernetes, which enables you to create a cluster of “nodes” to serve your containers among. To get started, create a cluster by following the prompts in the Kubernetes Engine dashboard. Then head to “workloads” and hit the “Deploy” button.

Make sure you click the “Select existing Google Container Registry image” button, and then choose your repository and tag (which will initially be v1). Click ‘Done’, give it a useful name, and then hit ‘Deploy’.

In the panel on the right, you will see the option to ‘Expose’ your image to the internet using a load balancer. Go ahead and do this. You will then be provided with an internet-facing endpoint which you can test your deployment on. Congratulations, you have just deployed a docker image to Google Cloud using Kubernetes Engine!

# Part 1 - Creating an R cluster


## Install Google Cloud SDK
```{bash, eval=F}
# Create environment variable for correct distribution
export CLOUD_SDK_REPO="cloud-sdk-$(lsb_release -c -s)"

# Add the Cloud SDK distribution URI as a package source
echo "deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list

# Import the Google Cloud Platform public key
curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

# Update the package list and install the Cloud SDK
sudo apt-get update && sudo apt-get install google-cloud-sdk

# Install Kubectl
sudo apt-get install kubectl


ls
```

```{bash}
gcloud compute instances list
```


### Configure kubectl to talk to the cluster

https://snyke.net/post/kubernetes-playground/
Finally we need to get the credentials for kubectl to talk to the cluster we just created

gcloud --project=$PROJECT_NAME container clusters get-credentials $CLUSTER_NAME \
       --zone=$ZONE
After that you should be able to call kubectl get nodes and see the virtual machines that comprise your cluster.

Setting the environment variables above will cause this step's entrypoint to first run a command to fetch cluster credentials as follows.

>gcloud container clusters get-credentials test-cluster --zone us-central1-a

Then, kubectl will have the configuration needed to talk to your GKE cluster.


### 
kubectl cluster-info

DR https://apache-spark-on-k8s.github.io/userdocs/running-on-kubernetes-cloud.html

### Install Spark

https://spark.apache.org/docs/2.3.0/running-on-kubernetes.html

### Install Spark Operator

At Google Cloud, we’ve used Operators to better support different applications on Kubernetes. For example, we have Operators for running and managing Spark and Airflow applications in a Kubernetes native way. We’ve also made these Operators available on the GCP Marketplace for an easy click-to-deploy experience. The Spark Operator automatically runs spark-submit on behalf of users, provides cron support for running Spark jobs on a schedule, supports automatic application restarts and re-tries and enables mounting data from local Hadoop configuration as well as Google Cloud Storage. The Airflow Operator creates and manages the necessary Kubernetes resources for an Airflow deployment and supports the creation of Airflow schedulers with different Executors.
## Connect

```{r}
library(sparklyr)
sparklyr:::get_java()
sc <- spark_connect(config = spark_config_kubernetes("k8s://35.239.34.73:8443"))
# Make sure JAVA_HOME is set
#readlink -f $(which java)
Sys.setenv(JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64/")
#sudo apt install default-jre
#sudo apt install default-jdk
#sudo R CMD javareconf
#
Sys.getenv("JAVA_HOME")
library(rJava)

#
Sys.setenv(JAVA_HOME="/usr/bin/java")
#https://github.com/rstudio/sparklyr
#Install latestes version
install.packages("devtools")
devtools::install_github("rstudio/sparklyr")
```



You may need the Java Development Kit (JDK) in addition to the JRE in order to compile and run some specific Java-based software. To install the JDK, execute the following command, which will also install the JRE:

$sudo apt install default-jdk

## Using
```{r eval=F}
library(googleKubernetesR)
```
## Or gcloud


Follow the [setup steps to authenticate with gcloud and kuberctl](https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster) then create your cluster via:




gcloud container clusters create r-cluster --num-nodes=3
…or if using googleKubernetesR :

Install via [github](https://github.com/RhysJackson/googleKubernetesR) 
```{r eval=F}
devtools::install_github("RhysJackson/googleKubernetesR")
```

```{r}
library(googleKubernetesR)
```
### Authenticate with JSON file
***
```{r eval=F}
k8s_auth(json = "client_secret.json")
```

## create a 3 node cluster called r-cluster with defaults
createCluster(projectId = gcp_get_project(), zone = "europe-west3-b")
You’ll need to wait a bit as it provisions all the VMs.

Most of the below will use the terminal/shell for working rather than R, but in the future a lot of this may be possible via googleKubernetesR within an R session.

Set up your shell to get the credentials for Kuberctl:

gcloud container clusters get-credentials r-cluster
And we are all set to start setting up this cluster for R jobs!

Here is a screenshot from the web UI of what it should look like:


#DElete
We’ll fit a simple model for illustrative purposes, using the Boston housing dataset (which ships with R in the MASS package). To make the deployment process more interesting, the model we fit will be a random forest, using the randomForest package. This is not part of R, so we’ll have to install it from CRAN.