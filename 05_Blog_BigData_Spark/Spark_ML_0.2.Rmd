---
output:
  html_document:
    theme: cerulean
    highlight: tango
    code_folding: show
    toc: yes
    toc_float: yes
  pdf_document:
    number_sections: yes
geometry: margin = 1.2in
fontsize: 10pt
always_allow_html: yes
---

```{r setup, include=FALSE}
# Time of computation below each chunk
knitr::knit_hooks$set(timeit = local({
  now = NULL
  function(before, options) {
    if (before) {
      now <<- Sys.time()
    } else {
      res = difftime(Sys.time(), now)
      now <<- NULL
      # use options$label if you want the chunk label as well
      paste('Chunk execution [min]:', as.character(res))
    }
  }})
)
```

```{r cache, include=FALSE}
#Caching of all chunks
knitr::opts_chunk$set(cache=TRUE)
```

# Use Case

In this case a Ubuntu DSVM is deployed. The
virtual machine is created within its own resource group so that all
created resources (the VM, networking, disk, etc) can be deleted
easily. Code is also included, and run, to then delete the
resource group if the resource group was created within this
vignette. Once deleted consumption (cost) will cease.

# Setup

To get started we need to load our Azure credentials as well as the
user's ssh public key. Public keys on Linux are typically created on
the users desktop/laptop machine and will be found within
~/.ssh/id_rsa.pub. It will be convenient to create a credentials file
to contain this information. 

#Authorization

We can simply source the credentials file in R.

```{r, eval=TRUE}
# Load the required subscription resources: TID, CID, and KEY.
# Also includes the ssh PUBKEY for the user.

USER <- Sys.info()[['user']]
#or set manually
USER<-"insider"

source("~/02_CloudComputing/10_Secrets/Azure_Authentication_AR.R")
```

If the required pacakges are not yet installed the following will do
so. You may need to install them into your own local library rather
than the system library if you are not a system user.



We can then load the required pacakges from the libraries.

# Set HDInsight

## Overview

`AzureSMR` provides an interface to manage resources on Microsoft Azure. The main functions address the following Azure Services:

- Azure Blob: List, Read and Write to Blob Services
- Azure Resources: List, Create and Delete Azure Resource
- Azure VM: List, Start and Stop Azure VMs
- Azure HDI: List and Scale Azure HDInsight Clusters
- Azure Hive: Run Hive queries against a HDInsight Cluster
- Azure Spark: List and create Spark jobs/Sessions against a HDInsight Cluster(Livy) - EXPERIMENTAL
- Azure Data Lake Store: ListStatus, GetFileStatus, MkDirs, Create (file), Append (file), Read (file), Delete


For a detailed list of `AzureSMR` functions and their syntax please refer to the Help pages.

## Configuring authorisation in Azure Active Directory

To get started, please refer to the [authorisation tutorial](http://htmlpreview.github.io/?https://github.com/Microsoft/AzureSMR/blob/master/inst/doc/Authentication.html)


## Load the package

```{r, eval=FALSE}
library(AzureSMR)
```

## Authenticating against the Azure service

The Azure APIs require many parameters to be managed. Rather than supplying all the arguments to every function call, `AzureSMR` uses an `azureActiveContext` object that caches arguments so you don't have to supply .

To create an `azureActiveContext` object and attempt to authenticate against the Azure service, use:

```{r, connect}
context <- createAzureContext(tenantID=TID, clientID=CID, authKey=KEY)
```

##Create a resource group

```{r}
azureCreateResourceGroup(context, resourceGroup = "R_Control", location = "centralus")
```

#Create Storage account

```{r}

azureCreateStorageAccount(context,storageAccount="testmystorage1",location = "centralus",resourceGroup = "R_Control")


```



## Manage HDInsight clusters

You can use `AzureSMR` to manage [HDInsight](https://azure.microsoft.com/en-gb/services/hdinsight/) clusters. To create a cluster use `azureCreateHDI()`.

For advanced configurations use Resource Templates (See below).
ca. 17 minutes
```{r}
azureCreateHDI(context,
                 resourceGroup = "R_Control",
                 clustername = "smrhdi", # only low case letters, digit, and dash.
                 storageAccount = "testmystorage1",
                 adminUser = "hdiadmin",
                 adminPassword = "AzureSMR_password123",
                 sshUser = "hdisshuser",
                 sshPassword = "AzureSMR_password123", 
                 kind = "rserver",
               location="centralus")
```

```{r}
library(jsonlite)

azR<-function(code){
a<-fromJSON(system(code,intern = TRUE))
return(a)
}

```

```{r}
b<-azR(paste("az group deployment create", 
       "--name insidedatascience",
       "--resource-group R_Control",
       "--template-file ~/01_Github/DannyJRa/05_Blog_BigData_Spark/template/template.json", 
       "--parameters ~/01_Github/DannyJRa/05_Blog_BigData_Spark/template/parameters.json"))
b

```

Use `azureListHDI()` to list available clusters.

```{r, eval=FALSE}
azureListHDI(context, resourceGroup ="R_Control")
```

Use `azureResizeHDI()` to resize a cluster
NOT_WORKING
```{r, eval=FALSE}
azureResizeHDI(context, resourceGroup = "R_control", clustername = "smrhdi", role="workernode",size=3)

## azureResizeHDI: Request Submitted:  2016-06-23 18:50:57
## Resizing(R), Succeeded(S)
## RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR
## RRRRRRRRRRRRRRRRRRS
## Finished Resizing Sucessfully:  2016-06-23 19:04:43
## Finished:  2016-06-23 19:04:43
##                                                                                                                        ## Information 
## " headnode ( 2 * Standard_D3_v2 ) workernode ( 5 * Standard_D3_v2 ) zookeepernode ( 3 * Medium ) edgenode0 ( 1 * Standard_D4_v2 )" 
```


ADMIN TIP: If a deployment fails, go to the Azure Portal and look at `Activity logs` and look for failed deployments - this should explain why the deployment failed.


## Hive Functions

You can use these functions to run and manage hive jobs on an HDInsight Cluster.

```{r, eval=FALSE}
azureHiveStatus(context, clusterName = "smrhdi", 
                hdiAdmin = "hdiadmin", 
                hdiPassword = "AzureSMR_password123")

azureHiveSQL(context, 
             CMD = "select * from hivesampletable", 
             path = "wasb://opendata@testmystorage1.blob.core.windows.net/")
```

## Spark functions (experimental)

`AzureSMR` provides some functions that allow HDInsight Spark aessions and jobs to be managed within an R Session.

To create a new Spark session (via [Livy](https://github.com/cloudera/hue/tree/master/apps/spark/java#welcome-to-livy-the-rest-spark-server)) use `azureSparkNewSession()`

```{r, eval=FALSE}
azureSparkNewSession(context, clustername = "spark1987", 
                     hdiAdmin = "insider_danny", 
                     hdiPassword = ".Alfonstini642856",
                     kind = "pyspark")
```

To view the status of sessions use `azureSparkListSessions()`. Wait for status to be idle.

```{r, eval=FALSE}
azureSparkListSessions(context, clustername = "spark1987")
```

To send a command to the Spark Session use `azureSparkCMD()`. In this case it submits a Python routine. Ensure you preserve indents for Python.

```{r}
# SAMPLE PYSPARK SCRIPT TO CALCULATE PI
pythonCmd <- '
from pyspark import SparkContext
from operator import add
import sys
from random import random
partitions = 1
n = 20000000 * partitions
def f(_):
  x = random() * 2 - 1
  y = random() * 2 - 1
  return 1 if x ** 2 + y ** 2 < 1 else 0
 
count = context.parallelize(range(1, n + 1), partitions).map(f).reduce(add)
Pi = (4.0 * count / n)
print("Pi is roughly %f" % Pi)'                   
 
azureSparkCMD(context, CMD = pythonCmd, sessionID = "0")

## [1] "Pi is roughly 3.140285"
```

Check Session variables are retained

```{r, eval=FALSE}
azureSparkCMD(context, clustername = "spark1987", CMD = "print Pi", sessionID = "0")

#[1] "3.1422"
```

You can also run SparkR sessions

NOT WORKING
```{r, eval=FALSE}
azureSparkNewSession(context, clustername = "spark1987", 
                     hdiAdmin = "", 
                     hdiPassword = "",
                     kind = "sparkr")


azureSparkCMD(context, clustername = "smrhdi", CMD = "HW<-'hello R'", sessionID = "2")
azureSparkCMD(context, clustername = "smrhdi", CMD = "cat(HW)", sessionID = "2")
```

