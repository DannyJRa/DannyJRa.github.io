---
title: "Untitled"
output: html_document
---

```{r}

library(reticulate)
#Define python environment
use_python("C:/Users/danny.rasch/Anaconda/python.exe")


#Test
os <- import("os")
default_path=os$getcwd()
```
Source: https://stackoverflow.com/questions/50085788/qt-platform-plugin-issue-rstudio
```{python}
import os
#BE
os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = 'C:/Users/danny.rasch/Anaconda/Library/plugins/platforms'


#os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = 'C:/Program Files (x86)/Microsoft Visual #Studio/Shared/Anaconda3_64/Library/plugins/platforms'


```


```{python pyplot, echo=FALSE}
import matplotlib
import matplotlib.pyplot as plt
import numpy as np

t = np.arange(0.0, 2.0, 0.01)
s = 1 + np.sin(2 * np.pi * t)

fig, ax = plt.subplots()
ax.plot(t, s)

ax.set(xlabel='time (s)', ylabel='voltage (mV)',
       title='About as simple as it gets, folks')
ax.grid()

plt.show()
```


```{python}
import pandas as pd
path="C:/OneDrive/7_DataScience/21_EC2/01_Github/DannyJRa/09_Reticulate/Chapter2/"
filename="Chapter 2\\titanic3.csv"
fullpath=r.default_path+'\\'+filename
data=pd.read_csv(fullpath)
print(data.head())
```



```{python}
import pandas as pd
path="C:/OneDrive/7_DataScience/21_EC2/01_Github/DannyJRa/09_Reticulate/Chapter2/"
filename="Chapter 2\\titanic3.csv"
fullpath=os.path.join(r.default_path,filename)
data=pd.read_csv(fullpath)
print(data.head())
print(data.columns.values)
print(data.dtypes)

```
```{python,echo=TRUE}
print(pd.isnull(data['body']).values.ravel().sum())
```

```{r}
test<-py$data
```

#Sourcing Python scripts

You can source any Python script just as you would source an R script using the source_python() function. For example, if you had the following Python script flights.py:

import pandas
def read_flights(file):
  flights = pandas.read_csv(file)
  flights = flights[flights['dest'] == "ORD"]
  flights = flights[['carrier', 'dep_delay', 'arr_delay']]
  flights = flights.dropna()
  return flights
Then you can source the script and call the read_flights() function as follows:

```{r}
source_python("flights.py")
flights <- read_flights("flights.csv")

library(ggplot2)
ggplot(flights, aes(carrier, arr_delay)) + geom_point() + geom_jitter()
```

#R Markdown Python Engine


## Python Version

By default, reticulate uses the version of Python found on your PATH (i.e. Sys.which("python")). If you want to use an alternate version you should add one of the use_python() family of functions to your R Markdown setup chunk, for example:

See the article on Python Version Configuration for additional details on configuring Python versions (including the use of conda or virtualenv environments).

```{r}
python<-Sys.which("python")
```

##Python Chunks

Python code chunks work exactly like R code chunks: Python code is executed and any print or graphical (matplotlib) output is included within the document.

Python chunks all execute within a single Python session so have access to all objects created in previous chunks. Chunk options like echo, include, etc. all work as expected.

Hereâ€™s an R Markdown document that demonstrates this:

```{python}

import os

import numpy as np
import matplotlib.pyplot as plt

X = np.linspace(-np.pi, np.pi, 256, endpoint=True)
C, S = np.cos(X), np.sin(X)

plt.plot(X, C)
plt.plot(X, S)

plt.show()
```

Requirement RStudio >v1.2

## Calling Python from R
All objects created within Python chunks are available to R using the py object exported by the reticulate package. For example, the following code demonstrates reading and filtering a CSV file using Pandas then plotting the resulting data frame using ggplot2:

#DJR https://rstudio.github.io/reticulate/articles/r_markdown.html

See the Calling Python from R article for additional details on how to interact with Python types from within R

## Calling R from Python

You can analagously access R objects within Python chunks via the r object. For example:

```{r}
library(tidyverse)
flights <- read_csv("flights.csv") %>% 
  filter(dest=="ORD") %>% 
  select(carrier, dep_delay,arr_delay) %>% 
  na.omit()
```


```{python}
print(r.flights.head())
```


#Chapter 1:
```{python}
from urllib.request import urlopen

html = urlopen('http://pythonscraping.com/pages/page1.html')
print(html.read())
```

```{python}
from urllib.request import urlopen
from bs4 import BeautifulSoup

html = urlopen('http://www.pythonscraping.com/pages/page1.html')
bs = BeautifulSoup(html.read(), 'html.parser') #html parser standard with python3
#bs = BeautifulSoup(html.read(), 'html5lib')
#bs = BeautifulSoup(html.read(), 'lxml')
print(bs.h1)
```

```{r}
title=py$bs
```


```{python}
from urllib.request import urlopen
from urllib.error import HTTPError
from urllib.error import URLError

try:
    html = urlopen("https://pythonscrapingthisurldoesnotexist.com")
except HTTPError as e:
    print("The server returned an HTTP error")
except URLError as e:
    print("The server could not be found!")
else:
    print(html.read())
```
# Error Handling

```{python}
from urllib.request import urlopen
from urllib.error import HTTPError
from bs4 import BeautifulSoup


def getTitle(url):
    try:
        html = urlopen(url)
    except HTTPError as e:
        return None
    try:
        bsObj = BeautifulSoup(html.read(), "lxml")
        title = bsObj.body.h1
    except AttributeError as e:
        return None
    return title


title = getTitle("http://www.pythonscraping.com/pages/page1.html")
if title == None:
    print("Title could not be found")
else:
    print(title)
```
#Chapter 2
```{python}
from urllib.request import urlopen
from bs4 import BeautifulSoup
html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')
bs = BeautifulSoup(html, "html.parser")
print(bs)
```

```{python}
nameList = bs.findAll('span', {'class': 'green'})
for name in nameList:
    print(name.get_text()) # strips all  tags
    

```

```{python}
titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6'])
print([title for title in titles])
```

```{python}
allText = bs.find_all('span', {'class':{'green', 'red'}})
print([text for text in allText])
```

```{python}
nameList = bs.find_all(text='the prince')
print(len(nameList))
```

```{python}
title = bs.find_all(id='title', class_='text')
print([text for text in allText])
```

```{python}
from urllib.request import urlopen
from bs4 import BeautifulSoup

html = urlopen('http://www.pythonscraping.com/pages/page3.html')
bs = BeautifulSoup(html, 'html.parser')

for child in bs.find('table',{'id':'giftList'}).children:
    print(child)
```
 Four objects available
 Childrean vs descendants
```{python}
from urllib.request import urlopen
from bs4 import BeautifulSoup

html = urlopen('http://www.pythonscraping.com/pages/page3.html')
bs = BeautifulSoup(html, 'html.parser')

for sibling in bs.find('table', {'id':'giftList'}).tr.next_siblings:
    print(sibling)
```

## Parents
```{python}
from urllib.request import urlopen
from bs4 import BeautifulSoup

html = urlopen('http://www.pythonscraping.com/pages/page3.html')
bs = BeautifulSoup(html, 'html.parser')
print(bs.find('img',
              {'src':'../img/gifts/img1.jpg'})
      .parent.previous_sibling.get_text())
```

##Regular expressions
```{python}
from urllib.request import urlopen
from bs4 import BeautifulSoup
import re

html = urlopen('http://www.pythonscraping.com/pages/page3.html')
bs = BeautifulSoup(html, 'html.parser')
images = bs.find_all('img', {'src':re.compile('\.\.\/img\/gifts/img.*\.jpg')})
for image in images: 
    print(image['src'])
```

```{python}
bs.find_all(lambda tag: len(tag.attrs) == 2)
```

```{python}
bs.find_all(lambda tag: tag.get_text() == 'Or maybe he\'s only resting?')
bs.find_all('', text='Or maybe he\'s only resting?')
```
#Chapter 3

```{python}
from urllib.request import urlopen
from bs4 import BeautifulSoup
html = urlopen('https://inside-datascience.com/')
bs = BeautifulSoup(html, 'html.parser')
for link in bs.find_all('a'):
  if 'href' in link.attrs:
    print(link.attrs['href'])
```

##Retriving articles only
```{python eval=FALSE, include=FALSE}
from urllib.request import urlopen 
from bs4 import BeautifulSoup 
import re

html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')
bs = BeautifulSoup(html, 'html.parser')
for link in bs.find('div', {'id':'bodyContent'}).find_all(
    'a', href=re.compile('^(/index/)((?!:).)*$')):
    if 'href' in link.attrs:
        print(link.attrs['href'])
```

```{python}
from urllib.request import urlopen
from bs4 import BeautifulSoup
import datetime
import random
import re

random.seed(datetime.datetime.now())
def getLinks(articleUrl):
    html = urlopen('https://inside-datascience.com/'.format(articleUrl))
    bs = BeautifulSoup(html, 'html.parser')
    return bs.find_all('a')

links = getLinks('/index')
while len(links) > 0:
    newArticle = links[random.randint(0, len(links)-1)].attrs['href']
    print(newArticle)
    links = getLinks(newArticle)
```

```{python}
from urllib.request import urlopen
from bs4 import BeautifulSoup
import re

pages = set()
def getLinks(pageUrl):
    global pages
    html = urlopen('https://inside-datascience.com/'.format(pageUrl))
    bs = BeautifulSoup(html, 'html.parser')
    try:
        print(bs.h1.get_text())
        print(bs.find(id ='mw-content-text').find_all('p')[0])
        print(bs.find(id='ca-edit').find('span').find('a').attrs['href'])
    except AttributeError:
        print('This page is missing something! Continuing.')
    
    for link in bs.find_all('a'):
        if 'href' in link.attrs:
            if link.attrs['href'] not in pages:
                #We have encountered a new page
                newPage = link.attrs['href']
                print('-'*20)
                print(newPage)
                pages.add(newPage)
                getLinks(newPage)
getLinks('')
```

```{python}
import requests
from bs4 import BeautifulSoup

def getPage(url):
    """
    Utilty function used to get a Beautiful Soup object from a given URL
    """

    session = requests.Session()
    headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36",
               "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"}
    try:
        req = session.get(url, headers=headers)
    except requests.exceptions.RequestException:
        return None
    bs = BeautifulSoup(req.text, "html.parser")
    return bs
```

```{python}
import requests


class Content:
    def __init__(self, url, title, body):
        self.url = url
        self.title = title
        self.body = body


def getPage(url):
    req = requests.get(url)
    return BeautifulSoup(req.text, 'html.parser')


def scrapeNYTimes(url):
    bs = getPage(url)
    title = bs.find("h1").text
    lines = bs.find_all("p", {"class": "story-content"})
    body = '\n'.join([line.text for line in lines])
    return Content(url, title, body)


def scrapeBrookings(url):
    bs = getPage(url)
    title = bs.find("h1").text
    body = bs.find("div", {"class", "post-body"}).text
    return Content(url, title, body)


url = 'https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'
content = scrapeBrookings(url)
print('Title: {}'.format(content.title))
print('URL: {}\n'.format(content.url))
print(content.body)

url = 'https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html'
content = scrapeNYTimes(url)
print('Title: {}'.format(content.title))
print('URL: {}\n'.format(content.url))
print(content.body)
```

```{python}
import scrapy
```
Chapter 6: Storing Data
```{python}
import psycopg2
conn = psycopg2.connect("host=localhost dbname=postgres user=postgres password=datascience64")
```

```{python}
from nltk import word_tokenize
from nltk import Text
tokens = word_tokenize('Here is some not very interesting text')
text = Text(tokens)
print(text)
```


```{python}
from nltk.book import *
print(len(text6)/len(set(text6)))
```

```{python}


```

